{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c65b13f-1747-473e-a77a-5b50b2ddc698",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fasttext\n",
    "import os\n",
    "import subprocess\n",
    "import yaml\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4e8f062-4c6a-411a-8e8f-c3213b023ab7",
   "metadata": {},
   "source": [
    "## config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fed3abd-b5ab-4cb8-90be-bac6a11c85f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train data \n",
    "TRAIN_DATA_PATH = os.getenv(\"train_data_path\")\n",
    "TRAIN_DATA_DESCRIPTION = os.getenv(\"train_data_description\")\n",
    "\n",
    "# model data\n",
    "TRAINING_ARCHITECTURE = os.getenv(\"training_architecture\")\n",
    "MODEL_ID = os.getenv(\"model_id\")\n",
    "MODEL_PATH = os.getenv(\"model_path\") + MODEL_ID + \"/\"\n",
    "MODEL_METADATA_PATH = MODEL_PATH + \"metadata.yaml\"\n",
    "\n",
    "# model hyperparameters\n",
    "VECTOR_SIZE = int(os.getenv(\"vector_size\"))\n",
    "EPOCHS=int(os.getenv(\"epochs\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "950a8577-9d9d-4172-b258-4f9e9c090199",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"TRAIN_DATA_PATH: {TRAIN_DATA_PATH}\")\n",
    "print(f\"TRAIN_DATA_DESCRIPTION: {TRAIN_DATA_DESCRIPTION}\")\n",
    "print(f\"TRAINING_ARCHITECTURE: {TRAINING_ARCHITECTURE}\")\n",
    "print(f\"MODEL_PATH: {MODEL_PATH}\")\n",
    "print(f\"MODEL_ID: {MODEL_ID}\")\n",
    "print(f\"MODEL_METADATA_PATH: {MODEL_METADATA_PATH}\")\n",
    "print(f\"VECTOR_SIZE: {VECTOR_SIZE}\")\n",
    "print(f\"EPOCHS: {EPOCHS}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c29c9ec2-b4aa-4eff-974a-95b29dfa9b4d",
   "metadata": {},
   "source": [
    "## training and persisting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dc6aa4c-fac2-441f-9ec5-ed5d867dd09d",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_start = datetime.now()\n",
    "print(time_start, flush=True)) # flush necessary for jupyter VM podman, to keep prints synchronized\n",
    "model = fasttext.train_unsupervised(TRAIN_DATA_PATH, epoch=EPOCHS, dim=VECTOR_SIZE)\n",
    "print(\"done\")\n",
    "time_end = datetime.now()\n",
    "print(time_end, flush=True))\n",
    "duration = (time_end - time_start).seconds / 60\n",
    "if not os.path.exists(MODEL_PATH):\n",
    "    os.makedirs(MODEL_PATH)\n",
    "model.save_model(MODEL_PATH + \"/model.bin\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46734bbf-d312-401b-95ab-2586a88f0d87",
   "metadata": {},
   "source": [
    "## writing metadata "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd682376-83c8-455a-a14e-708b6787ca47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate size of training and model data\n",
    "def calc_size(file_or_folder):\n",
    "    size = subprocess.run([\"du\", \"-sh\", file_or_folder], capture_output=True, text=True)\n",
    "    size = size.stdout.split()[0]\n",
    "    return size\n",
    "train_data_size = calc_size(TRAIN_DATA_PATH)\n",
    "model_data_size = calc_size(MODEL_PATH)\n",
    "\n",
    "# calculate hash of training data\n",
    "train_data_md5_hash = subprocess.run([\"md5sum\", TRAIN_DATA_PATH], capture_output=True, text=True)\n",
    "train_data_md5_hash = train_data_md5_hash.stdout.split()[0]\n",
    "\n",
    "# aggregate into metadata dictionary\n",
    "metadata = {\n",
    "    \"training_architecture\": TRAINING_ARCHITECTURE,\n",
    "    \"model_id\": MODEL_ID, \n",
    "    \"train_data_name\": TRAIN_DATA_DESCRIPTION,\n",
    "    \"train_data_size\": train_data_size,\n",
    "    \"train_data_md5_hash\": train_data_md5_hash,\n",
    "    \"training_epochs\": EPOCHS,\n",
    "    \"training_vector_size\": VECTOR_SIZE,\n",
    "    \"training_duration (minutes)\": round(duration, 1),\n",
    "    \"model_data_size\": model_data_size,\n",
    "}\n",
    "\n",
    "# write to yaml\n",
    "with open(MODEL_METADATA_PATH, \"w\") as f:\n",
    "    # iteration over dictionary to ensure the yaml writer respects the order\n",
    "    for k, v in metadata.items():\n",
    "        yaml.dump({k: v}, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
